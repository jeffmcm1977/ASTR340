{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17083941",
   "metadata": {},
   "source": [
    "# Moments and Correlation\n",
    "\n",
    "## You must master the definitions of the following terms\n",
    "(We will review these cocnepts in class.  The following references go beyond what will be covered and include much useful inforamtion. )\n",
    "1. [moments](https://en.wikipedia.org/wiki/Moment_(mathematics))\n",
    "1. [correlation](https://en.wikipedia.org/wiki/Correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37836764",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91631425",
   "metadata": {},
   "source": [
    "\n",
    "# Moments\n",
    "\n",
    "In the last example from the previous notebook (Statistical Distribtions) we saw that the chi-squared distribtion does not have the same shape as the gaussian PDF.   We would like a tool to quantify these differences.  Moments give us jsut such a tool.  These can be used to summarize (compress) the information contained in a PDF.  The mean and variance are the most famous examples.  The genral equaiton for moments is:\n",
    "\n",
    "#### \n",
    "\n",
    "\\begin{equation}\n",
    "\\langle x^n \\rangle  = \\rm{E(x^n)} = \\displaystyle\\int_{-\\infty}^{\\infty} x^n \\phi(x) dx\n",
    "\\label{eq:moments} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\langle x^n \\rangle  = \\rm{E(x^n)}$ is defined as the expectation value. n = 1,2,3 are the first, second, third, etc moments.  The probability density fuciton $\\phi$ is assumed to be normalized such that $\\int \\phi dx = 1$.\n",
    "\n",
    "The first moment is just the (PDF) weighted mean. \n",
    "\n",
    "<i> Centered moments </i> are higher moments shifted to this mean: $\\mu^n = \\langle (x - \\langle x \\rangle )^n \\rangle$\n",
    "\n",
    "An important centered moment is for n = 2: $\\mu^2 = \\langle (x - \\langle x \\rangle )^2 \\rangle$. This is the <i>variance</i>, which is the square of the spread about the mean. One can easily show that:\n",
    "\n",
    "$$\\mu^2 = E(x^2) - (E(x))^2$$\n",
    "\n",
    "## Example\n",
    "\n",
    "One can use the above techniques to calculate the variance about the mean of the <i>Poisson Distribution</i> and find:\n",
    "\n",
    "$$\\mu^2 = \\sigma^2 = \\sqrt{\\lambda}$$\n",
    "\n",
    "where we have switched to the more common variance label, $\\sigma^2$. It is usefull to remmeber that the variance of a Poisson distribution is the square-root of the mean.  If you have evidence that your data were generated through a Poisson process (or you measure a Poisson process), you will often see means and errors quoted as $m \\pm{\\sqrt{m}}$ (e.g. photon counting with $m$ counts). \n",
    "\n",
    "Verify this numerically. I have given you a realizaiton of the poisson distirbtion below.  Vary the lambda paramter and possibly the number of samples and check that the variance and mean are consistant with this formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b860d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate a realization of N samples following a poisson distribtion with zero mean and unit variance\n",
    "N = 50\n",
    "data = np.random.poisson(9,N)   ###<<<<<<<<<<<<<<<< modify this line\n",
    "\n",
    "## plot this realization \n",
    "plt.plot(data,\",k\")\n",
    "plt.title(\"Samples from the Poisson PDF\")\n",
    "plt.xlabel(\"sample (integers)\")\n",
    "plt.ylabel(\"value (arb)\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## compute the mean and standard deviation and print them out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91355839",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Higher moments -- left in as review, but covered last lecture\n",
    "The third moment, called the skewness, $‚ü®x^3‚ü©$, characterizes the simplest asymmetry, while the fourth moment, the kurtosis, $‚ü®x^4‚ü©$, characterizes the flatness of the distribution.\n",
    "\n",
    "### calculating moments from samples of a distribtion\n",
    "\n",
    "If you are working form a set of samples of a distribution $x_i$ for $i = 1... N$ then the moments are calculated as $$<x> = \\frac 1 N \\Sigma_{i=1}^N x_i, $$ and for centered moments with $n>1$ $$ E(\\mu^n) = \\frac 1 {N-1} \\Sigma_{i=1}^N (x_i-<x>)^n .$$\n",
    "\n",
    "### variance vs variance on the mean\n",
    "\n",
    "The second moment is the sample varaince which gives us an estimate of the width of the population from which individual samples are drawn from.   Often times we are interested in a related question which is this: if we calucate the mean using $N$ samples from this polulation, what is the variance on this measurement of the mean?  In other words, if we were to repeat the measuremnt of the mean multiple times, what would the standard deviation of the measurments of the mean be?   The variance on the mean can be calulated as $\\sigma_{mean}= \\frac {\\sigma_{sample}}{\\sqrt{N-1}} $ where $N$ is the number of samples.  The factor $N-1$ acounts for the fact that the average is removed reducing the number of degrees of freedom.   We will use this below when plotting error bars.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a169a94",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Generate a realization of the Poisson distribtion with 10000 saples and lamda = 10.  Compute the mean, standard deviation, [skewness](https://en.wikipedia.org/wiki/Skewness), and [kertossis](https://en.wikipedia.org/wiki/Kurtosis).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb960bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c1ff4",
   "metadata": {},
   "source": [
    "discussion goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48eb64a",
   "metadata": {},
   "source": [
    "## Multiple independant random processes\n",
    "\n",
    "Often multiple random porcesses, perhaps obeying seperate statistical distribtions will contribute to the description of the data.  A question that comes up a lot is: \"what happens to the momnents\" in cases where multiple sources of errors are present.  The following math proves the rule that for statistically indpendant quantities, the moments of the sum is the sum of the moments.\n",
    "\n",
    "\\begin{align*} \n",
    "\t\tE(X+Y) &= \\sum_{x} \\sum_{y} (x+y) p(x, y)\\\\ \n",
    "\t\t&= \\sum_{x} \\sum_{y}\\left(x p(x, y) + y p(x, y)\\right)\\\\ \n",
    "\t\t&= \\sum_{x} \\sum_{y} x p(x, y) + \\sum_{x} \\sum_{y} y p(x, y)\\\\ \n",
    "\t\t&= \\sum_x x \\sum_y p(x,y) + \\sum_y y \\sum_x p(x,y)\\\\ \n",
    "\t\t&= \\sum x p_X(x) + \\sum_y y p_Y(y)\\\\ \n",
    "\t\t&= E(X) + E(Y) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "For example if X and Y are gaussians with unit varaince and we compute the variance of the sum we would get $\\sigma^2 = 2$.  Since the RMS = $\\sqrt{\\sigma^2}$ this shows the rule that ading equal errors increases the error by $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d0437",
   "metadata": {},
   "source": [
    "### Excercise: \n",
    "\n",
    "Imagine a detector with an intrinsic noise described by a gaussian of mean zero and unit variance.  Call this distribtion $x_1$   If this detector is pointed at the sky, atmospheric emission could contribute a statisticaly indepndant signal described by poisson statisstics.  Call this signal $x_2$ and assume $\\lambda = 0.2$.  \n",
    "\n",
    "Compute the mean and the variance for these distribtins individually and for the sum.  Confirm that the moments of the sum is the sum of the moments for these two cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54289fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb5bdd",
   "metadata": {},
   "source": [
    "## Correlation and Covariance\n",
    "\n",
    "#### \n",
    "Consider 4 coins and 5 flips each and we record the results:\n",
    "\n",
    "Coin ùê¥:+,‚àí,‚àí,+,‚àí\n",
    "\n",
    "Coin ùêµ:‚àí,‚àí,‚àí,‚àí,+\n",
    "\n",
    "Coin ùê∂:+,‚àí,+,+,‚àí\n",
    "\n",
    "Coin ùê∑:‚àí,+,‚àí,‚àí,+\n",
    "\n",
    "Which coins are correlated and by how much?\n",
    "\n",
    "##### \n",
    "We define a function that is positive when the two results are similar, and negative when they are dissimilar. The easiest function is multiplication: it will be positive when the coins have the same result (++ or --) and negative when they differ (+- and -+). \n",
    "\n",
    "We can multiply each trial, $\\it{then\\ average\\ the\\ results}$ to get an overall estimate of how similar the results are.\n",
    "\n",
    "$\\begin{align}\n",
    "C_{AB} &= \\frac{-1+1+1-1-1}{5} = 0.2 \\tag{5}\\\\\n",
    "C_{AC} &= \\frac{+1+1-1+1+1}{5} = 0.6 \\tag{6}\\\\\n",
    "C_{AD} &= \\frac{-1-1+1-1-1}{5} = -0.6 \\tag{7} \\\\\n",
    "\\end{align}$\n",
    "\n",
    "So ùê¥ and ùêµ seem pretty uncorrelated, ùê¥ and ùê∂ may be correlated, and ùê¥ and ùê∑ are anti-correlated.\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "For bivariate distributions (of two random variables), consider the probaility distribution $\\phi(x,y)$ where the random variables $X$ and $Y$ are <i>NOT</i> independent, one can also define a covariance (assuming $‚ü®x‚ü© = ‚ü®y‚ü© = 0$):\n",
    "\n",
    "$$Cov(x,y) = \\langle xy \\rangle = \\int_{-\\infty}^{\\infty} xy\\ \\phi(xy)\\ dxdy = E\\big[(xy)\\big] \\tag{8}$$\n",
    "\n",
    "This leads to a formal definition of the correlation coefficient:\n",
    "\n",
    "$$r = \\frac{Cov(x,y)}{Var(x)Var(y)} \\tag{9}$$\n",
    "\n",
    "where $Var$ is the variance as defined above. \n",
    "\n",
    "with more than two variables are present we often subscript $r$ as $r_{ij}$ where $i$ and $j$ specify the two channels being considered.  The collection of all possible $r_{ij}$ elements represtns the covaraince (or correleation) matrix.\n",
    "\n",
    "### Equivelent Definitions\n",
    "Sometimes, you will see an equivalent definiton using vectors (for centered data):\n",
    "\n",
    "$$r = cos\\theta = \\frac{\\vec{a} \\cdot \\vec{b}}{|a||b|}$$\n",
    "\n",
    "If the data are not centered, then we have:\n",
    "$$Cov(x,y) = E\\big[(x-\\mu_x)(y-\\mu_y)\\big]$$\n",
    "\n",
    "and\n",
    "\n",
    "$$r = \\frac{E\\big[(x-\\mu_x)(y-\\mu_y)\\big]}{\\sqrt{\\mu^2_x\\mu^2_y}}$$\n",
    "\n",
    "which is nominally known as the <i> Pearson correlation coefficient </i>.\n",
    "\n",
    "In practice for a sample of $x,y$ pairs, we write:\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{N}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}} \\tag{10}$$\n",
    "\n",
    "Covariance can range from -$\\infty$ to +$\\infty$. But Correlation ranges from -1 to +1. Unlike the correlation, the magnitude of the covariance does not mean anything since it is dependent upon the magnitude of the constituent of the series. However in both cases, the sign DOES matter. \n",
    "\n",
    "To be clear, <i>correlation is the covariance</i>, but normalized to the sample variances. Because it is normalized, the value of the correlation becomes a meaningful quantity to compare to other correlations.\n",
    "\n",
    "There is a fun article which presents 13 different (!) interpretations of correlation [here](https://www.stat.berkeley.edu/~rabbee/correlation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c51bc",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "Consider the three timesreams given below $x$, $y$, and $z$.  I have given you the computation of a covariance and a correlation between one set of channel pairs as a starting point.    \n",
    "\n",
    "1. How may difference correlations and covariance are there to compute? \n",
    "1. Compute the covariance, and the correlation between all sets of channels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 100000\n",
    "A = np.random.randn(N)\n",
    "B = np.random.randn(N)\n",
    "C = np.random.randn(N)\n",
    "\n",
    "x = A +2*B\n",
    "y = -2*A - 2*B -1*C\n",
    "z=  C\n",
    "\n",
    "\n",
    "plt.plot(x,\",\")\n",
    "plt.plot(y,\",\")\n",
    "plt.plot(z,\",\")\n",
    "plt.title(\"our three data sets\")\n",
    "plt.ylabel(\"value (arb)\")\n",
    "plt.xlabel(\"sample (int)\")\n",
    "plt.show()\n",
    "\n",
    "## compute your answeres here.\n",
    "\n",
    "cov_xz = np.sum(x*z)\n",
    "cor_xz = np.sum(x*z) / np.sqrt(np.sum(x*x)*np.sum(z*z))\n",
    "print(\"covariance xz: \",cov_xz, \"correlation_xz\", cor_xz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fe5e0-60f0-438d-9126-e3ccdfbab36a",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "Plot the x vs y, x vs z, and y vs z to observe the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37162a6-9022-4337-928c-0e7ec045f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4316139e-afc4-4bd3-b38b-29656dff19ea",
   "metadata": {},
   "source": [
    "## Bivariate Gaussian distributions\n",
    "\n",
    "The generalization of the Gaussian distrubution is defined as\n",
    "    \n",
    "$$ p(x,y|\\mu_x, \\mu_y, \\sigma_x,\\sigma_y,\\sigma_{xy}) = \\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{(1-\\rho^2)}}\\text{exp}{\\bigg(\\frac{-z^2}{2(1-\\rho^2)}\\bigg)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ z^2 = \\frac{(x-\\mu_x)^2}{\\sigma_x^2}+\\frac{(y-\\mu_y)^2}{\\sigma_y} -2\\rho\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y} $$\n",
    "\n",
    "and the correlation coefficient between x and y is defined as \n",
    "$$ \\rho = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y} $$\n",
    "\n",
    "For perfectly correlated variables, $\\rho = \\pm 1$ and for uncorrelated variables, $\\rho = 0.$\n",
    "\n",
    "## Excercise\n",
    "\n",
    "Use this functional form to plot one sigma error contours over the distribtuions you found in the pervious excericise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f8f37-b5e5-4ba1-85c5-b4f5d7165639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "## convert data from 1d arrays to a Nx2 array as spected by the histogramdd function\n",
    "tmp  = np.transpose(np.stack((x,y),axis=0))\n",
    "print(np.shape(tmp))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot a 2D histogram/hess diagram of the points\n",
    "H, bins = np.histogramdd(tmp, bins=2 * [np.linspace(-7, 7, 51)])\n",
    "ax.imshow(H, origin='lower', cmap=plt.cm.binary, interpolation='nearest',\n",
    "          extent=[bins[0][0], bins[0][-1], bins[1][0], bins[1][-1]])\n",
    "\n",
    "# draw 1, 2, 3-sigma ellipses over the distribution\n",
    "mean = np.array([0, 0])\n",
    "sigma_1 = 2\n",
    "sigma_2 = 1\n",
    "alpha = np.pi / 4\n",
    "\n",
    "for N in (1, 2, 3):\n",
    "    ax.add_patch(Ellipse(mean, N * sigma_1, N * sigma_2,\n",
    "                         angle=alpha * 180. / np.pi, lw=1,\n",
    "                         ec='k', fc='none'))\n",
    "\n",
    "kwargs = dict(ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.text(0.02, 0.98, r\"$\\sigma_1 = %i$\" % sigma_1, **kwargs, fontsize = 12)\n",
    "ax.text(0.02, 0.93, r\"$\\sigma_2 = %i$\" % sigma_2, **kwargs, fontsize = 12)\n",
    "ax.text(0.02, 0.88, r\"$\\alpha = \\pi / %i$\" % (np.pi / alpha), **kwargs, fontsize = 12)\n",
    "\n",
    "\n",
    "ax.set_xlabel('$x$', fontsize = 16)\n",
    "ax.set_ylabel('$y$', fontsize = 16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c705d",
   "metadata": {},
   "source": [
    "## Excerecise \n",
    "\n",
    "In the previous excercise you shold have figured out that three are 9 correlation coefficeincs.   Fill in the followign tables for the covariance and correlaiton matrixes.  We will use thsese structures often in multivariate statistics.\n",
    "\n",
    "|  \\    | $\\sigma_x$ | $\\sigma_y$ | $\\sigma_z$ | \n",
    "| --- | --- | --- | --- |\n",
    "|$\\sigma_x$ | X | X | X |\n",
    "|$\\sigma_y$ | X | X | X |\n",
    "|$\\sigma_z$ | X | X | X |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cad9be-f343-43a7-961c-5d86841e94b9",
   "metadata": {},
   "source": [
    "## A differnt type of correlation\n",
    "\n",
    "Correlations needent be described with simple relationships in the x-y plane.  Observe the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369da10d-2de6-4017-b6cb-a6d4ee0ad179",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = np.random.rand(1000) * 2* np.pi\n",
    "delta_r = np.random.randn(1000) * .1\n",
    "\n",
    "\n",
    "alpha = (1 +delta_r)*np.cos(psi)\n",
    "beta = (1 +delta_r)*np.sin(psi)\n",
    "\n",
    "plt.plot(alpha,\",\")\n",
    "plt.plot(beta,\",\")\n",
    "plt.title(\"our two data sets\")\n",
    "plt.ylabel(\"value (arb)\")\n",
    "plt.xlabel(\"sample (int)\")\n",
    "plt.show()\n",
    "\n",
    "## compute your answeres here.\n",
    "\n",
    "cov_xz = np.sum(alpha*beta)\n",
    "cor_xz = np.sum(alpha*beta) / np.sqrt(np.sum(alpha*alpha)*np.sum(beta*beta))\n",
    "print(\"covariance xz: \",cov_xz, \"correlation_xz\", cor_xz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada03cd9-5afb-447c-8697-95da994a5a73",
   "metadata": {},
   "source": [
    "## excercise\n",
    "\n",
    "Plot alpha vs beta and observe the correlation.   \n",
    "\n",
    "Note-- the correlation coefficient is zero in these coordinates. \n",
    "\n",
    "Given what you see in the plot how could you switch varibales to make the correlation easier to quantify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f55f6-6a55-47a4-903e-85318bfe9340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
